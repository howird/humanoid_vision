"""Detection data structures used throughout the PHALP tracking pipeline."""

from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Sequence

import numpy as np
from numpy import ndarray

from jaxtyping import Float, jaxtyped
from beartype import beartype

from humanoid_vision.common.types import (
    AppearanceEmbed,
    CameraBBox,
    CameraTranslation,
    WeakPerspCamera,
    Joints3DWithRoot,
    TrackingEmbed,
    FocalLength,
    Joints2DFlatWithRoot,
    LocationEmbed,
    PoseEmbed,
    UVMap,
)


@jaxtyped(typechecker=beartype)
@dataclass
class Detection:
    """Rich per-human observation emitted by PHALP.

    Each attribute is annotated with its *source model* and *downstream usage*:

    ``bbox``
        `(x, y, w, h)` from the Detectron2 `DefaultPredictor_Lazy`. Used by
        the Deep SORT gating logic, PHALP visualisation and for cropping inputs
        to HMAR.
    ``mask``
        RLE-encoded mask provided by Detectron2. Stored for rendering and for
        downstream datasets that expect segmentation labels.
    ``conf``
        Detector confidence score from Detectron2. Drives track confirmation
        thresholds and weighting of UV fusion in `Track.update`.
    ``appe``
        Appearance embedding from the HMAR autoencoder. Fed into the
        `NearestNeighborDistanceMetric` for track association.
    ``pose``
        Pose embedding derived from HMAR SMPL outputs. Used alongside ``appe``
        for distance computation and to seed the pose predictor.
    ``loca``
        Location embedding concatenating 2D joints and weak-perspective camera
        parameters from HMAR. Consumed by the motion predictor for future
        localisation.
    ``uv``
        UV texture map from HMAR. Required for UV-based appearance blending and
        optional rendering stages.
    ``embedding``
        Concatenated `(appe, pose, loca)` vector retained for legacy consumers
        (e.g. feature dumps).
    ``center`` / ``scale``
        Center and scale of the padded bounding box (pre-HMAR). Generated by
        the crop utility and reused when projecting joints back to image
        coordinates.
    ``smpl``
        Dictionary of SMPL parameters from HMAR (`body_pose`, `betas`,
        `global_orient`). Required when re-projecting or exporting tracks.
    ``camera``
        Weak-perspective camera flattened from HMAR (`pred_cam`). Consumed by
        the location predictor and output writers.
    ``camera_bbox``
        Original camera parameters before flattening; useful for debugging
        calibration differences per bounding box.
    ``joints_3d`` / ``joints_2d``
        Joint positions from HMAR in 3D and normalised 2D space respectively.
        Used by downstream pose evaluators and predictors.
    ``size``
        Original frame height/width. Needed to recover pixel coordinates.
    ``img_path`` / ``img_name``
        Frame provenance, forwarded to loggers and exporters.
    ``class_name``
        Detectron2 class index (0 for person). Preserved for completeness.
    ``time``
        Frame index within the processed clip. Drives temporal ordering.
    ``ground_truth`` / ``annotations``
        Optional GT track linkage and annotation payloads used in evaluation.
    ``hmar_out_cam`` / ``hmar_out_cam_t`` /
    ``hmar_out_focal_length``
        Additional HMAR camera tensors (translation + focal length) retained
        for advanced reprojection tasks.

    Two derived attributes are computed for downstream predictors:

    ``xy``
        Normalised centre of the bounding box within the square-padded image.
        Required by the pose transformer when constructing temporal windows.
    ``normalized_scale``
        Largest side of the padded bounding box divided by the padded image
        size. Used by the location predictor to preserve scale invariance.
    """

    bbox: Float[ndarray, "4"]
    mask: list[dict[str, Any]]
    conf: float
    appe: AppearanceEmbed
    pose: PoseEmbed
    loca: LocationEmbed
    uv: UVMap
    embedding: TrackingEmbed
    center: Float[ndarray, "2"]
    scale: Float[ndarray, "2"]
    smpl: dict[str, np.ndarray]
    camera: WeakPerspCamera
    camera_bbox: CameraBBox
    joints_3d: Joints3DWithRoot
    joints_2d: Joints2DFlatWithRoot
    size: Sequence[int]
    img_path: str | Path | None
    img_name: str | None
    class_name: int
    time: int
    ground_truth: int | None
    annotations: list[Any]
    hmar_out_cam: WeakPerspCamera
    hmar_out_cam_t: CameraTranslation
    hmar_out_focal_length: FocalLength

    tlwh: Float[ndarray, "4"] = field(init=False)
    xy: Float[ndarray, "2"] = field(init=False)
    normalized_scale: float = field(init=False)

    def __post_init__(self) -> None:
        self.bbox = np.asarray(self.bbox, dtype=np.float64)
        self.tlwh = self.bbox.copy()

        self.appe = np.asarray(self.appe, dtype=np.float32)
        self.pose = np.asarray(self.pose, dtype=np.float32)
        self.loca = np.asarray(self.loca, dtype=np.float32)
        self.uv = np.asarray(self.uv)
        self.embedding = np.asarray(self.embedding)
        self.center = np.asarray(self.center, dtype=np.float32)
        self.scale = np.asarray(self.scale, dtype=np.float32)
        self.camera = np.asarray(self.camera, dtype=np.float32)
        self.camera_bbox = np.asarray(self.camera_bbox, dtype=np.float32)
        self.joints_3d = np.asarray(self.joints_3d, dtype=np.float32)
        self.joints_2d = np.asarray(self.joints_2d, dtype=np.float32)
        self.hmar_out_cam = np.asarray(self.hmar_out_cam, dtype=np.float32)
        self.hmar_out_cam_t = np.asarray(self.hmar_out_cam_t, dtype=np.float32)
        self.hmar_out_focal_length = np.asarray(
            self.hmar_out_focal_length, dtype=np.float32
        )

        self.size = tuple(int(v) for v in self.size)
        img_height, img_width = map(float, self.size)
        new_image_size = max(img_height, img_width)
        delta_w = new_image_size - img_width
        delta_h = new_image_size - img_height
        top = delta_h // 2
        left = delta_w // 2

        self.xy = np.asarray(
            [
                ((self.tlwh[0] + self.tlwh[2]) / 2.0 + left) / new_image_size,
                ((self.tlwh[1] + self.tlwh[3]) / 2.0 + top) / new_image_size,
            ],
            dtype=np.float32,
        )

        self.normalized_scale = float(np.max(self.scale)) / new_image_size

    @jaxtyped(typechecker=beartype)
    def to_tlbr(self) -> Float[ndarray, "4"]:
        """Return `(x1, y1, x2, y2)` bounding box coordinates."""

        ret = self.tlwh.copy()
        ret[2:] += ret[:2]
        return ret

    @jaxtyped(typechecker=beartype)
    def to_xyah(self) -> Float[ndarray, "4"]:
        """Return `(cx, cy, aspect_ratio, height)` representation."""

        ret = self.tlwh.copy()
        ret[:2] += ret[2:] / 2
        ret[2] /= ret[3]
        return ret

    def as_legacy_dict(self) -> dict[str, Any]:
        """Match the legacy dictionary payload for downstream consumers."""

        return {
            "bbox": self.tlwh.copy(),
            "mask": self.mask,
            "conf": float(self.conf),
            "appe": np.asarray(self.appe),
            "pose": np.asarray(self.pose),
            "loca": np.asarray(self.loca),
            "uv": np.asarray(self.uv),
            "embedding": np.asarray(self.embedding),
            "center": np.asarray(self.center),
            "scale": self.normalized_scale,
            "smpl": self.smpl,
            "camera": np.asarray(self.camera),
            "camera_bbox": np.asarray(self.camera_bbox),
            "3d_joints": np.asarray(self.joints_3d),
            "2d_joints": np.asarray(self.joints_2d),
            "size": np.asarray(self.size),
            "img_path": self.img_path,
            "img_name": self.img_name,
            "class_name": self.class_name,
            "time": self.time,
            "ground_truth": self.ground_truth,
            "annotations": self.annotations,
            "hmar_out_cam": np.asarray(self.hmar_out_cam),
            "hmar_out_cam_t": np.asarray(self.hmar_out_cam_t),
            "hmar_out_focal_length": np.asarray(self.hmar_out_focal_length),
            "xy": np.asarray(self.xy),
        }
